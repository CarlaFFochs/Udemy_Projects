{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Structured Data Project 2: Predicting the sale price of Bulldozers (regression)",
      "provenance": [],
      "authorship_tag": "ABX9TyNa0uEszIcOxYxuzmDTMAde",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CarlaFFochs/Udemy_Projects/blob/main/Structured_Data_Project_2_Predicting_the_sale_price_of_Bulldozers_(regression).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predicting the Sale Price of Bulldozers using Machine Learining\n",
        "\n",
        "In this notebook we're going to go through an example machine learning project with the goal of predicting the sale price of bulldozers.\n",
        "\n",
        "### 1. Problem definition\n",
        "\n",
        "> How well can we predict the future sale price of a bulldozer, given its characterisitics and previous examples of how much similiar bulldozers have been sold for? \n",
        "\n",
        "### 2. Data\n",
        "\n",
        "The data is downloaded from Kaggle Competition: https://www.kaggle.com/c/bluebook-for-bulldozers/data\n",
        "\n",
        "  The data for this competition is split into three parts:\n",
        "\n",
        "  * Train.csv is the training set, which contains data through the end of 2011.\n",
        "  * Valid.csv is the validation set, which contains data from January 1, 2012 - April 30, 2012 You make predictions on this set throughout the majority of the competition. Your score on this set is used to create the public leaderboard.\n",
        "  *Test.csv is the test set, which won't be released until the last week of the competition. It contains data from May 1, 2012 - November 2012. Your score on the test set determines your final rank for the competition.\n",
        "\n",
        "### 3. Evalutation\n",
        "\n",
        "The evaluation metric for this competition is the RMSLE (root mean squared log error) between the actual and predicted auction prices. For more info check: https://www.kaggle.com/c/bluebook-for-bulldozers/overview/evaluation\n",
        "\n",
        "**Note:** The goal for most regression evalutation metrics is to minimize the error. For example, the goal for this project will be to build a ML model which minimises RMSLE.\n",
        "\n",
        "### 4. Features\n",
        "\n",
        "Kaggle provides a data dictionary detailing all the features of the data set: https://docs.google.com/spreadsheets/d/1Mhm5o1ZXLt2o-uE2GPq506iyoJDpoQX_/edit?usp=sharing&ouid=104958891807092880404&rtpof=true&sd=true\n"
      ],
      "metadata": {
        "id": "_h2ZYV5zwOVS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WT4-YxRn8FL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import training and  validation sets\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/MASTER DATA SCIENCE/M0/M0 - UDEMY/Time Series (Supervised Learning)/data/bluebook-for-bulldozers/TrainAndValid.csv\",\n",
        "                 low_memory=False)\n",
        "\n",
        "#no cal que minimitzem el espai (low_memory=False)"
      ],
      "metadata": {
        "id": "vHkkyfLZsG9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "pN3NkCne4iSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum() #mirem els valors nuls"
      ],
      "metadata": {
        "id": "GhSidqV75e_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "UMA3cRQL5-X6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.scatter(df[\"saledate\"][:1000], df[\"SalePrice\"][:1000])"
      ],
      "metadata": {
        "id": "kw0nplVc45-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.saledate[:1000] #no ens agrada el format de la data"
      ],
      "metadata": {
        "id": "Z_LV274P6GOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.SalePrice.plot.hist()"
      ],
      "metadata": {
        "id": "NmZJPQP_6cpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parsing dates\n",
        "\n",
        "When we work with time series data, we want to enrich the time & date component as much as possible.\n",
        "\n",
        "We can do that by telling pandas which of our columns has dates in it using \"parse_dates\" parameters. "
      ],
      "metadata": {
        "id": "ZyiHnjL0Iunu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import data again but this time parse dates\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/MASTER DATA SCIENCE/M0/M0 - UDEMY/Time Series (Supervised Learning)/data/bluebook-for-bulldozers/TrainAndValid.csv\",\n",
        "                 low_memory=False, parse_dates=[\"saledate\"])"
      ],
      "metadata": {
        "id": "f6rjAflfJCee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"saledate\"].dtype # és equivalent al \"datetime64[ns]\""
      ],
      "metadata": {
        "id": "iiKJ03gKJWn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"saledate\"] # gràcies al \"parse_date\" ens ho ha passat al format internacional YYYY-MM-DD"
      ],
      "metadata": {
        "id": "1cym2nTtJY7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.scatter(df[\"saledate\"][:1000], df[\"SalePrice\"][:1000])"
      ],
      "metadata": {
        "id": "sr8p3Gu-HkrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sort DataFrame by saledate\n",
        "\n",
        "When working with time series data, it's a good idea to sort it by date."
      ],
      "metadata": {
        "id": "X5YEKPjVGgKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort DataFrame in date order\n",
        "df.sort_values(by=[\"saledate\"], inplace=True, ascending=True)\n",
        "df.saledate.head(20)"
      ],
      "metadata": {
        "id": "bg3_fp5_Gndt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Make a copy of the original DataFrame\n",
        "\n",
        "We make a copy of the original datagrame so when we manipulate the copy, we've still got our original data."
      ],
      "metadata": {
        "id": "gdxjWcmSKIVu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_tmp= df.copy()"
      ],
      "metadata": {
        "id": "fdhOBAquLPge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tmp.saledate.head(20)"
      ],
      "metadata": {
        "id": "Mu_s8EwDLSSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add datetime parameters for 'saledate' column"
      ],
      "metadata": {
        "id": "VFGd1us5LVNe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_tmp[:1][\"saledate\"]"
      ],
      "metadata": {
        "id": "KwNUGKlmMw-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tmp[:1][\"saledate\"].dt.year"
      ],
      "metadata": {
        "id": "UJ-HkJy4L9rd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tmp[:1][\"saledate\"].dt.day"
      ],
      "metadata": {
        "id": "Rajx_OsIMq4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tmp[\"saleYear\"]= df_tmp[\"saledate\"].dt.year\n",
        "df_tmp[\"saleMonth\"]= df_tmp[\"saledate\"].dt.month\n",
        "df_tmp[\"saleDay\"]= df_tmp[\"saledate\"].dt.day\n",
        "df_tmp[\"saleDayofWeek\"]= df_tmp[\"saledate\"].dt.dayofweek\n",
        "df_tmp[\"saleDayofYear\"]= df_tmp[\"saledate\"].dt.dayofyear"
      ],
      "metadata": {
        "id": "tfRYlB41M0d-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tmp.T"
      ],
      "metadata": {
        "id": "Lcht_oxONA3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Podem veure que s'han afegit les columnes al final del dataframe\n",
        "# No necessitem ja el \"saledate\""
      ],
      "metadata": {
        "id": "Uw1sAyAmNAtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we've enriched our DataFrame with date time features, we cann remove \"saledate\"\n",
        "\n",
        "df_tmp.drop(\"saledate\", axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "PfMnbDzNNAGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the values of different columns\n",
        "df_tmp[\"state\"].value_counts() #obtenim la llista de les ventes"
      ],
      "metadata": {
        "id": "KB0Synr1M_f5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df_tmp)"
      ],
      "metadata": {
        "id": "CQFMYtqEO2H3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.Modelling\n",
        "\n",
        "We've done enough EDA (we could always do more) but let's start to do some model-driven EDA."
      ],
      "metadata": {
        "id": "vmWyVnp1OB0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's build a ML model\n",
        "#from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "#model= RandomForestRegressor(n_jobs=-1,\n",
        "#                             random_state=42)\n",
        "\n",
        "#model.fit(df_tmp.drop(\"SalePrice\", axis=1), df_tmp[\"SalePrice\"])\n",
        "\n",
        "\n",
        "# ERROR que ens surt:\n",
        "# could not convert string to float: 'Low'"
      ],
      "metadata": {
        "id": "2oMjHvMdOfSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tmp[\"UsageBand\"].dtype "
      ],
      "metadata": {
        "id": "rLrwQvoePTLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "zlLr_rouPVSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert string into categories\n",
        "\n",
        "One way we can turn our data into numbers is by converting them into pandas categories."
      ],
      "metadata": {
        "id": "q7SrraY1Pe7O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_tmp.head().T"
      ],
      "metadata": {
        "id": "qfwfYx6gStIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.api.types.is_string_dtype(df_tmp[\"UsageBand\"])"
      ],
      "metadata": {
        "id": "4HoNJBsBStAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the columns which contain strings\n",
        "\n",
        "for label, content in df_tmp.items():\n",
        "  if pd.api.types.is_string_dtype(content):\n",
        "    print(label)"
      ],
      "metadata": {
        "id": "C6ZPJi70Ss9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tenim totes les columnes que tenen strings"
      ],
      "metadata": {
        "id": "fDtSOsG2Ss6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If you are wondering what df.items() does, here's an example:\n",
        "\n",
        "random_dict = {\"key1\": \"hello\",\n",
        "               \"key2\": \"world\"}\n",
        "\n",
        "for key, value in random_dict.items():\n",
        "  print(f\"this is a key:  {key}\",\n",
        "        f\"this is a value: {value}\")"
      ],
      "metadata": {
        "id": "n9Buoc-QSs04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This will turn all of the string values to category values\n",
        "\n",
        "for label, content in df_tmp.items():\n",
        "  if pd.api.types.is_string_dtype(content):\n",
        "    df_tmp[label] = content.astype(\"category\").cat.as_ordered()"
      ],
      "metadata": {
        "id": "2IMtptJvUhZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tmp.info()"
      ],
      "metadata": {
        "id": "VMHQu8eHVjFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tmp.state.cat.categories #estan ordenades amb el cat.as_ordered(), pero pandas esta tractant com a números, les categories sónn números. Per cada etiqueta asigna un número, ho comprovem abaix"
      ],
      "metadata": {
        "id": "ahv5r4W-Vk6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tmp.state.cat.codes #mirem quin número se li ha asignat per cada \"state\""
      ],
      "metadata": {
        "id": "BXLTTtrqVx3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thanks to pandas Categories we now have a way to acces all of our data in a form of numbers, but we still have a bunch of missing data..."
      ],
      "metadata": {
        "id": "0R30wF8eWJJh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check missing data\n",
        "\n",
        "df_tmp.isnull().sum()/len(df_tmp)"
      ],
      "metadata": {
        "id": "WUn97PPmWoXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save preprocessed data"
      ],
      "metadata": {
        "id": "OELGxZJBWodC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Export current tmp dataframe\n",
        "# Guardem el dataframe manipulat\n",
        "\n",
        "df_tmp.to_csv(\"/content/drive/MyDrive/MASTER DATA SCIENCE/M0/M0 - UDEMY/Time Series (Supervised Learning)/data/train_tmp.csv\",\n",
        "              index=False)"
      ],
      "metadata": {
        "id": "mYYKhxqeWohw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import preprocessed data\n",
        "\n",
        "df_tmp = pd.read_csv(\"/content/drive/MyDrive/MASTER DATA SCIENCE/M0/M0 - UDEMY/Time Series (Supervised Learning)/data/train_tmp.csv\",\n",
        "              low_memory=False)"
      ],
      "metadata": {
        "id": "1NNvacxHWom2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tmp.head().T"
      ],
      "metadata": {
        "id": "nEDwi9hnYEt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fill missing values\n",
        "\n",
        "###Fill numerical values\n"
      ],
      "metadata": {
        "id": "x9_FMchPYEqy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for label, content in df_tmp.items():\n",
        "  if pd.api.types.is_numeric_dtype(content):\n",
        "    print(label)"
      ],
      "metadata": {
        "id": "KPjvR8oXYEn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tmp.ModelID"
      ],
      "metadata": {
        "id": "NdaMIc5CYEh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for which numeric columns have null values\n",
        "\n",
        "for label, content in df_tmp.items():\n",
        "  if pd.api.types.is_numeric_dtype(content):\n",
        "    if pd.isnull(content).sum(): #els valors que tenen la suma de nuls superiors a 0, sino directament ja no fa la suma\n",
        "      print(label)"
      ],
      "metadata": {
        "id": "3GzZ2ZAqYtgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill numeric rows with the median\n",
        "\n",
        "for label, content in df_tmp.items():\n",
        "  if pd.api.types.is_numeric_dtype(content):\n",
        "    if pd.isnull(content).sum(): #els valors que tenen la suma de nuls superiors a 0, sino directament ja no fa la suma\n",
        "      # Add a binay column which tells us if the data is missing (per saber que inicialment hi havia un valor que faltava)\n",
        "      df_tmp[label+\"_is_missing\"] = pd.isnull(content)\n",
        "      #Fill missing numeric values with the mdeianl\n",
        "      df_tmp[label] = content.fillna(content.median()) #es millor la mediana que la media"
      ],
      "metadata": {
        "id": "RfI_zB1wZSfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Demonstrate how median is more robust than mean\n",
        "\n",
        "hundreds = np.full((1000,), 100)\n",
        "hundreds_billion = np.append(hundreds, 1000000000)\n",
        "np.mean(hundreds), np.mean(hundreds_billion), np.median(hundreds), np.median(hundreds_billion)"
      ],
      "metadata": {
        "id": "TqUq7zNrZ4Ew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if there's any null numeric values\n",
        "\n",
        "for label, content in df_tmp.items():\n",
        "  if pd.api.types.is_numeric_dtype(content):\n",
        "    if pd.isnull(content).sum():\n",
        "      print(label)"
      ],
      "metadata": {
        "id": "ZjoSzodJa22S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check to see what the binary column has done\n",
        "df_tmp.auctioneerID_is_missing.value_counts()"
      ],
      "metadata": {
        "id": "QcOentsddhdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hem rellenat 20136 valors amb la median"
      ],
      "metadata": {
        "id": "e4MEGWVWdGiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tmp.isna().sum() # encara hem de omplir els missing values (de les categories...)"
      ],
      "metadata": {
        "id": "GlUEMxQUd3nT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filling and turning categorical variables into numbers"
      ],
      "metadata": {
        "id": "waX50ztid5Ji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for columns which aren't numeric\n",
        "\n",
        "for label, content in df_tmp.items():\n",
        "  if not pd.api.types.is_numeric_dtype(content):\n",
        "    print(label)"
      ],
      "metadata": {
        "id": "um8mZLxzmRAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.Categorical(df_tmp[\"state\"]).dtype"
      ],
      "metadata": {
        "id": "oh-kkzenmRFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.Categorical(df_tmp[\"UsageBand\"]).codes "
      ],
      "metadata": {
        "id": "kpXL4uclo0hu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn categorical variables into numbers and fill missing\n",
        "\n",
        "for label, content in df_tmp.items():\n",
        "  if not pd.api.types.is_numeric_dtype(content):\n",
        "    # Add binary column to indicate wheter sample had missing value\n",
        "    df_tmp[label+\"is_missing\"] = pd.isnull(content)\n",
        "    # Turn categories into numbers and add +1\n",
        "    df_tmp[label] = pd.Categorical(content).codes + 1"
      ],
      "metadata": {
        "id": "A0kask4Jm14m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.Categorical(df_tmp[\"UsageBand\"]).codes # si hi ha una categoria que te un missing value, li asigna directament un -1 (pero nosaltres volem que sigui 0)"
      ],
      "metadata": {
        "id": "cuqjBZaKmRP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.Categorical(df_tmp[\"state\"]).codes"
      ],
      "metadata": {
        "id": "4ll0vFgNnYQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.Categorical(df_tmp[\"state\"]).codes +1"
      ],
      "metadata": {
        "id": "U5A5LpP3mRLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tmp.info()"
      ],
      "metadata": {
        "id": "nu0uXjiPsgLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tmp.head().T"
      ],
      "metadata": {
        "id": "fZqDPpd4nj_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tmp.isna().sum()[:20]"
      ],
      "metadata": {
        "id": "pDbXg3gFsE7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that all data is numerica as well as our dateframe has no missing values, we should be able to build a ML model."
      ],
      "metadata": {
        "id": "KpN20PaZsQtW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_tmp.head()"
      ],
      "metadata": {
        "id": "kR7oWkMRtV2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df_tmp)"
      ],
      "metadata": {
        "id": "2kW6dazptcWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Instantiate model\n",
        "model = RandomForestRegressor(n_jobs=-1,\n",
        "                              random_state=42)\n",
        "\n",
        "# Fit the model\n",
        "model.fit(df_tmp.drop(\"SalePrice\", axis=1), df_tmp[\"SalePrice\"])"
      ],
      "metadata": {
        "id": "u0lb-pmotYDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Score the model\n",
        "model.score(df_tmp.drop(\"SalePrice\", axis=1), df_tmp[\"SalePrice\"])"
      ],
      "metadata": {
        "id": "yujuPJ6St-rx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:** Why does't the above metric hold water? (why isn't the metric reliable)"
      ],
      "metadata": {
        "id": "Y0-8f_xCvz4n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ens dona una score tant alta perquè hem EVALUAT el model amb la mateixes dades que el TRAINING set.\n",
        "\n",
        "# Hem ampres els materials de la classe, però en comptes de fer un examen final (preguntes noves), ens evaluen exactament de les mateixes preguntes extretes del llibre que has llegit abans de fer l'examen.\n",
        "# Esta bé, però nosaltres el que busquem la capacitat de que el nostre model per generalitzar (habilitat d'un model de ML que performi bé amb data que mai ha vist)\n"
      ],
      "metadata": {
        "id": "KSHLsczow6iH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Splitting data into train/validation sets"
      ],
      "metadata": {
        "id": "2XK2G8vrwHlH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_tmp.saleYear"
      ],
      "metadata": {
        "id": "f92PYkm-wHcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tmp.saleYear.value_counts()"
      ],
      "metadata": {
        "id": "I8OGF8L2wHSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training and validation\n",
        "df_val = df_tmp[df_tmp.saleYear == 2012]\n",
        "df_train = df_tmp[df_tmp.saleYear != 2012]\n",
        "\n",
        "len(df_val), len(df_train)"
      ],
      "metadata": {
        "id": "UBkp7ZoZxCju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into X & y\n",
        "X_train, y_train = df_train.drop(\"SalePrice\", axis=1), df_train[\"SalePrice\"]\n",
        "X_valid, y_valid = df_val.drop(\"SalePrice\", axis=1), df_val[\"SalePrice\"]\n",
        "\n",
        "X_train.shape, y_train.shape, X_valid.shape, y_valid.shape"
      ],
      "metadata": {
        "id": "D3RNciQyxmq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "id": "2KN2YZxyyGWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building an evalutation function\n"
      ],
      "metadata": {
        "id": "1byD2SE7yQeY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create evalutation function (the competition uses RMSLE)\n",
        "from sklearn.metrics import mean_squared_log_error, mean_absolute_error, r2_score\n",
        "\n",
        "def rmsle(y_test,y_preds):\n",
        "  \"\"\"\n",
        "  Calculates root mean squared log error between predictions and true lables\n",
        "  \"\"\"\n",
        "  return np.sqrt(mean_squared_log_error(y_test, y_preds))\n",
        "\n",
        "# Create function to evaluate model on a few different levels\n",
        "def show_scores(model):\n",
        "  train_preds = model.predict(X_train)\n",
        "  val_preds = model.predict(X_valid) # si aqui ho ha millor, ens dona una pista que el model té overfitting - normalment el validation set té una pitjor performance\n",
        "  scores = {\"Training MAE\": mean_absolute_error(y_train, train_preds),\n",
        "            \"Valid MAE\": mean_absolute_error(y_valid, val_preds),\n",
        "            \"Training RMSLE\": rmsle(y_train, train_preds),\n",
        "            \"Valid RMSLE\": rmsle(y_valid, val_preds),\n",
        "            \"Training R^2\": r2_score(y_train, train_preds),\n",
        "             \"Valid R^2\": r2_score(y_valid, val_preds)}\n",
        "\n",
        "  return scores"
      ],
      "metadata": {
        "id": "Dzn1uS390ocD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing our model on a subset (to tune the hyperparameters)"
      ],
      "metadata": {
        "id": "cXklVFzi0om5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This takes far too long...for experimenting\n",
        "\n",
        "# %%time\n",
        "# model = RandomForestRegressor(n_jobs=-1,\n",
        "#                              random_state=42)\n",
        "# model.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "2OjuPE6v0or9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_train)"
      ],
      "metadata": {
        "id": "CaVEegCi0ov4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change max_samples value\n",
        "model = RandomForestRegressor(n_jobs=-1,\n",
        "                            random_state=42,\n",
        "                            max_samples=10000)\n",
        "model"
      ],
      "metadata": {
        "id": "z5XZzeoA3tH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cutting down on the max_number of samples each estimator can see improves training time\n",
        "%%time\n",
        "model.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "O562AQzr6fG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape[0]"
      ],
      "metadata": {
        "id": "y3N_c5oO4MnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_scores(model)"
      ],
      "metadata": {
        "id": "4qzVpbQS4xKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameter tunning with RandomizedSearchCV\n",
        "\n",
        "Randomized search on hyperparameters."
      ],
      "metadata": {
        "id": "UxFrXr-E61-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Different RandomForestRegressor hyperparameters\n",
        "rf_grid = {\"n_estimators\": np.arange(10,100,10),\n",
        "           \"max_depth\": [None, 3, 5, 10],\n",
        "           \"min_samples_split\": np.arange(2,20,2),\n",
        "           \"min_samples_leaf\": np.arange(1,20,2),\n",
        "           \"max_features\": [0.5,1, \"sqrt\", \"auto\"],\n",
        "           \"max_samples\": [10000]}\n",
        "\n",
        "# Instantiate RandomizedSearch CV model\n",
        "rs_model = RandomizedSearchCV(RandomForestRegressor(n_jobs=-1,\n",
        "                                                    random_state=42),\n",
        "                              param_distributions=rf_grid,\n",
        "                              n_iter=2,\n",
        "                              cv=5,\n",
        "                              verbose=True)\n",
        "\n",
        "# Fit the RandomizedSearchCV model (farà un fit de 2 iteracions, 2 combinacions de parametrres)\n",
        "rs_model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "LoFrSLJe9HVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the best hyperparameters\n",
        "\n",
        "rs_model.best_params_"
      ],
      "metadata": {
        "id": "j-tRYmjp9HMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Nomes hem buscat 2 combinacions.. no seran els millors\n",
        "\n",
        "show_scores(rs_model)"
      ],
      "metadata": {
        "id": "IXfmcYBpA2s8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Veiem que el RMSLE ha empitjorat una mica, però quadra perquè nomes hem fet 2 iteracions per buscar els millors parametres..."
      ],
      "metadata": {
        "id": "PePdBcpBBBZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train a model with the best hyperparameters\n",
        "\n",
        "**Note:** These where found after 100 iterations of RandomizedSearchCV\n"
      ],
      "metadata": {
        "id": "mQDWumddEWce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# Most ideal hyperparameters\n",
        "ideal_model = RandomForestRegressor(max_depth= 10,\n",
        "                                    max_features= 0.5,\n",
        "                                    max_samples= None, #perque agafi tot el model\n",
        "                                    min_samples_leaf= 15,\n",
        "                                    min_samples_split= 18,\n",
        "                                    n_estimators= 30,\n",
        "                                    n_jobs=-1)\n",
        "\n",
        "# Fit the ideal model\n",
        "ideal_model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "n39q7EtPEWht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scores for ideal_model (trained on all the data)\n",
        "\n",
        "show_scores(ideal_model)"
      ],
      "metadata": {
        "id": "jl15acL_EWmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scores for rs_model (trained on 10.000 examples)\n",
        "\n",
        "show_scores(rs_model)"
      ],
      "metadata": {
        "id": "VeLlqk_BFvjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make predictions on test data"
      ],
      "metadata": {
        "id": "moLLAKKXGAQM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the test data\n",
        "\n",
        "df_test = pd.read_csv(\"/content/drive/MyDrive/MASTER DATA SCIENCE/M0/M0 - UDEMY/Time Series (Supervised Learning)/data/bluebook-for-bulldozers/Test.csv\",\n",
        "                      low_memory = False,\n",
        "                      parse_dates= [\"saledate\"])\n",
        "df_test.head()"
      ],
      "metadata": {
        "id": "EVSZTBCPHlhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.columns"
      ],
      "metadata": {
        "id": "pRBt1mAORMln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predicitons on the test dataset\n",
        "# test_preds = ideal_model.predict(df_test)\n",
        "\n",
        "# ValueError: could not convert string to float: 'Low'"
      ],
      "metadata": {
        "id": "cXzPWFXmIAP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.isna().sum() #tenim valors nuls.."
      ],
      "metadata": {
        "id": "c7OY8EwyIQd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info() #no es tot numeric"
      ],
      "metadata": {
        "id": "tfzLyM4JIuYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# No esta al mateix format que el training set, ho hem de preprocessar com abans..."
      ],
      "metadata": {
        "id": "EFMbqYT4IyJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing the data (gettind the test dataset in the same format as our training dataset)"
      ],
      "metadata": {
        "id": "l3qJPp6-I-Q_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(df):\n",
        "  \"\"\"\n",
        "  Performs transformations on df and returns transformed df.\n",
        "  \"\"\"\n",
        "  df[\"saleYear\"]= df[\"saledate\"].dt.year\n",
        "  df[\"saleMonth\"]= df[\"saledate\"].dt.month\n",
        "  df[\"saleDay\"]= df[\"saledate\"].dt.day\n",
        "  df[\"saleDayofWeek\"]= df[\"saledate\"].dt.dayofweek\n",
        "  df[\"saleDayofYear\"]= df[\"saledate\"].dt.dayofyear\n",
        "\n",
        "  df.drop(\"saledate\", axis=1, inplace=True)\n",
        "\n",
        "  # Fill the numeric rows with median\n",
        "  for label, content in df.items():\n",
        "    if pd.api.types.is_numeric_dtype(content):\n",
        "      if pd.isnull(content).sum(): #els valors que tenen la suma de nuls superiors a 0, sino directament ja no fa la suma\n",
        "        # Add a binay column which tells us if the data is missing (per saber que inicialment hi havia un valor que faltava)\n",
        "        df[label+\"_is_missing\"] = pd.isnull(content)\n",
        "        # Fill missing numeric values with the mdeianl\n",
        "        df[label] = content.fillna(content.median()) #es millor la mediana que la media\n",
        "\n",
        "  # Fill categorical missing data and turn categories into numbers\n",
        "  for label, content in df.items():\n",
        "    if not pd.api.types.is_numeric_dtype(content):\n",
        "      # Add binary column to indicate wheter sample had missing value\n",
        "      df[label+\"is_missing\"] = pd.isnull(content)\n",
        "      # We add +1 to the category code because pandas encodes missinf categories as -1\n",
        "      df[label] = pd.Categorical(content).codes + 1\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "F1632UTGJEG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.columns"
      ],
      "metadata": {
        "id": "na6t8GtfQSgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process the test data\n",
        "df_test = preprocess_data(df_test)"
      ],
      "metadata": {
        "id": "l1mp6bA3JEK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.head()"
      ],
      "metadata": {
        "id": "QaU4YVnlJEO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "zK0o5d7rJESu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on updated test data\n",
        "# test_preds= ideal_model.predict(df_test)"
      ],
      "metadata": {
        "id": "icfV1LsvJEWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# No tenen la mateixa shape el train i test set...\n",
        "# We can find how the columns differ using sets\n",
        "set(X_train.columns) - set(df_test.columns)"
      ],
      "metadata": {
        "id": "zWzx85hbJEaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_test no té la columna 'auctioneerID_is_missing'\n",
        "\n",
        "# Manually adjust df_test to have auctioneerID_is_missing\n",
        "df_test[\"auctioneerID_is_missing\"]= False #no tenia missing values\n",
        "df_test.head()"
      ],
      "metadata": {
        "id": "79-ORFGswr3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally our test dataframe has the same features as our training data frame. we can make predicitons!"
      ],
      "metadata": {
        "id": "eZmlJbOMxE-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_preds = ideal_model.predict(df_test)"
      ],
      "metadata": {
        "id": "Nv-b051axMXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_preds"
      ],
      "metadata": {
        "id": "Du_pSVZJxQlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We've made some predicitions but they're not in the same format Kaggle  is asking for: https://www.kaggle.com/c/bluebook-for-bulldozers/overview/evaluation"
      ],
      "metadata": {
        "id": "BgJvGuLIzF77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Format predicitons into the same format Kaggle:\n",
        "df_preds= pd.DataFrame()\n",
        "df_preds[\"SalesID\"] = df_test[\"SalesID\"] #fem la nova columna \"SalesID\"\n",
        "df_preds[\"SalesPrice\"] = test_preds # creem la nova columna \"SalesPrice\", els valors son el resultats del test_preds\n",
        "df_preds"
      ],
      "metadata": {
        "id": "sUweXLKRy-8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Export prediciton data\n",
        "\n",
        "df_preds.to_csv(\"/content/drive/MyDrive/MASTER DATA SCIENCE/M0/M0 - UDEMY/Time Series (Supervised Learning)/data/test_predicitons\", index= False) #al arxiu li direm \"test_predictions\""
      ],
      "metadata": {
        "id": "vTUicTkXzt4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature importance"
      ],
      "metadata": {
        "id": "VlRt0haR0fP4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature importance seeks to figure out wich different attributes of the data were most important when it comes to predictinf the **target variable** (SalesPrice)"
      ],
      "metadata": {
        "id": "0MJ8PV_o04wT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find feature importance of our best model\n",
        "ideal_model.feature_importances_"
      ],
      "metadata": {
        "id": "E3WMeNnB1GLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(ideal_model.feature_importances_)"
      ],
      "metadata": {
        "id": "DGeTot1-1GPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "QHomkVCh1GUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "id": "FlHAjY8d1GZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are getting a value for each feature"
      ],
      "metadata": {
        "id": "cOxuQ8_-1Ge-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function for plotting feature importance\n",
        "\n",
        "def plot_features(columns, importances, n=20): #the tiop 20 values\n",
        "  df= (pd.DataFrame({\"features\": columns,\n",
        "                    \"feature_importances\": importances}).sort_values(\"feature_importances\", ascending = False).reset_index(drop=True))\n",
        "\n",
        "# Plot the dataframe\n",
        "  fig, ax = plt.subplots() # instantiate the plot\n",
        "  ax.barh(df[\"features\"][:n], df[\"feature_importances\"][:20])\n",
        "  ax.set_ylabel(\"Features\")\n",
        "  ax.set_xlabel(\"Feature importance\")\n",
        "  ax.invert_yaxis() #perque surti de més a menys (descending)"
      ],
      "metadata": {
        "id": "_C2-7W1j1ncC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_features(X_train.columns, ideal_model.feature_importances_)"
      ],
      "metadata": {
        "id": "2OZXryl41n8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"ProductSize\"].value_counts()"
      ],
      "metadata": {
        "id": "Fko4iyRt1n1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Enclosure\"].value_counts()"
      ],
      "metadata": {
        "id": "-tdXvtMm1nwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question to finish:** Why might knowing feature importances of a trained data be important\n",
        "\n",
        "**Final challenge:** What order machine learing models could we try on our dataset? Hint: https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html or try to look at something like CatBoost.ai or XGBoost.ai."
      ],
      "metadata": {
        "id": "mmEQBlE61niG"
      }
    }
  ]
}